{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import joblib\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_PROFILE\"] = \"park_pulse\" \n",
    "\n",
    "TRACKING_SERVER_HOST = \"ec2-54-90-62-154.compute-1.amazonaws.com\"\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://ec2-54-90-62-154.compute-1.amazonaws.com:5000'\n"
     ]
    }
   ],
   "source": [
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='s3://parkpulse-mlflow-artifacts/0', creation_time=1717404637620, experiment_id='0', last_update_time=1717404637620, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.search_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/03 12:02:33 INFO mlflow.tracking.fluent: Experiment with name 'version 1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://parkpulse-mlflow-artifacts/1', creation_time=1717408953570, experiment_id='1', last_update_time=1717408953570, lifecycle_stage='active', name='version 1', tags={}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"version 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/checkpoint5_neighbourhood_district_data.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "features = df[['icon', 'moon_phase', 'neighbourhood_bcn', 'precipitation', 'visibility', 'cloud_cover', 'snow', 'temp', 'is_weekend']]\n",
    "target = df['estimated_occupancy_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the selected columns\n",
    "df = df[features.columns.tolist() + [target.name]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "df[['precipitation', 'visibility', 'cloud_cover', 'snow', 'temp']] = scaler.fit_transform(df[['precipitation', 'visibility', 'cloud_cover', 'snow', 'temp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for weighted average\n",
    "weights = {'estimated_occupancy_rate': 0.5, 'precipitation': 0.15, 'visibility': 0.1, 'cloud_cover': 0.1, 'is_weekend': 0.1, 'snow': 0.1, 'temp': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted average for estimated occupancy rate\n",
    "df['weighted_occupancy'] = (\n",
    "    df['estimated_occupancy_rate'] * weights['estimated_occupancy_rate'] +\n",
    "    df['precipitation'] * weights['precipitation'] +\n",
    "    df['visibility'] * weights['visibility'] +\n",
    "    df['cloud_cover'] * weights['cloud_cover'] +\n",
    "    df['is_weekend'] * weights['is_weekend'] +\n",
    "    df['snow'] * weights['snow'] +\n",
    "    df['temp'] * weights['temp']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable for model training\n",
    "features = df[['icon', 'moon_phase', 'neighbourhood_bcn', 'weighted_occupancy']]\n",
    "target = df['estimated_occupancy_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df[features.columns]\n",
    "y = df[target.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['weighted_occupancy', 'moon_phase']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['icon', 'neighbourhood_bcn'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/03 12:05:52 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "# enable autologging\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"RandomForest\")\n",
    "        mlflow.log_params(params)\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomForestRegressor(\n",
    "                n_estimators=int(params['n_estimators']),\n",
    "                max_depth=int(params['max_depth']),\n",
    "                min_samples_split=int(params['min_samples_split']),\n",
    "                min_samples_leaf=int(params['min_samples_leaf']),\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        # Fit the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        # Perform cross-validation\n",
    "        # score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        # rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.sklearn.log_model(pipeline, artifact_path=\"models_pickle\")\n",
    "\n",
    "        # Save the preprocessor\n",
    "        preprocessor_filename = 'scales/preprocessor.pkl'\n",
    "        joblib.dump(preprocessor, preprocessor_filename)\n",
    "        mlflow.log_artifact(local_path=\"scales/preprocessor.pkl\", artifact_path=\"scales_pickle\")\n",
    "    return {'accuracy': r2, 'loss': mse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 20, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 2, 10, 1),\n",
    "    'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 10, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [22:43<00:00, 27.26s/trial, best loss: 0.0032213210243405527]\n",
      "Best hyperparameters: {'max_depth': 18.0, 'min_samples_leaf': 1.0, 'min_samples_split': 3.0, 'n_estimators': 102.0}\n"
     ]
    }
   ],
   "source": [
    "# Run Hyperopt optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=search_space,\n",
    "                   algo=tpe.suggest,\n",
    "                   max_evals=50,\n",
    "                   trials=trials)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save testing data for further inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('data/test_inference_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv('data/groundtruth_inference_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park-pulse-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
